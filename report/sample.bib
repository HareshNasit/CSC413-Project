Generated using https://www.bibtex.com/c/arxiv-to-bibtex-converter/

@Comment [1] in Google Docs
@ARTICLE{Liu2017-wl,
  title         = "Unsupervised image-to-image translation networks",
  author        = "Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan",
  abstract      = "Unsupervised image-to-image translation aims at learning a
                   joint distribution of images in different domains by using
                   images from the marginal distributions in individual
                   domains. Since there exists an infinite set of joint
                   distributions that can arrive the given marginal
                   distributions, one could infer nothing about the joint
                   distribution from the marginal distributions without
                   additional assumptions. To address the problem, we make a
                   shared-latent space assumption and propose an unsupervised
                   image-to-image translation framework based on Coupled GANs.
                   We compare the proposed framework with competing approaches
                   and present high quality image translation results on
                   various challenging unsupervised image translation tasks,
                   including street scene image translation, animal image
                   translation, and face image translation. We also apply the
                   proposed framework to domain adaptation and achieve
                   state-of-the-art performance on benchmark datasets. Code and
                   additional results are available in
                   https://github.com/mingyuliutw/unit .",
  month         =  mar,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1703.00848"
}

@Comment [2] in Google Docs
@ARTICLE{kazemi,
  title         = "Unsupervised image-to-image translation using
                   domain-specific variational information bound",
  author        = "Kazemi, Hadi and Soleymani, Sobhan and Taherkhani, Fariborz
                   and Iranmanesh, Seyed Mehdi and Nasrabadi, Nasser M",
  abstract      = "Unsupervised image-to-image translation is a class of
                   computer vision problems which aims at modeling conditional
                   distribution of images in the target domain, given a set of
                   unpaired images in the source and target domains. An image
                   in the source domain might have multiple representations in
                   the target domain. Therefore, ambiguity in modeling of the
                   conditional distribution arises, specially when the images
                   in the source and target domains come from different
                   modalities. Current approaches mostly rely on simplifying
                   assumptions to map both domains into a shared-latent space.
                   Consequently, they are only able to model the
                   domain-invariant information between the two modalities.
                   These approaches usually fail to model domain-specific
                   information which has no representation in the target
                   domain. In this work, we propose an unsupervised
                   image-to-image translation framework which maximizes a
                   domain-specific variational information bound and learns the
                   target domain-invariant representation of the two domain.
                   The proposed framework makes it possible to map a single
                   source image into multiple images in the target domain,
                   utilizing several target domain-specific codes sampled
                   randomly from the prior distribution, or extracted from
                   reference images.",
  month         =  nov,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1811.11979"
}

@Comment [3] in Google Docs
@ARTICLE{zhu,
  title         = "Unpaired image-to-image translation using cycle-consistent
                   adversarial networks",
  author        = "Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros,
                   Alexei A",
  abstract      = "Image-to-image translation is a class of vision and graphics
                   problems where the goal is to learn the mapping between an
                   input image and an output image using a training set of
                   aligned image pairs. However, for many tasks, paired
                   training data will not be available. We present an approach
                   for learning to translate an image from a source domain $X$
                   to a target domain $Y$ in the absence of paired examples.
                   Our goal is to learn a mapping $G: X \rightarrow Y$ such
                   that the distribution of images from $G(X)$ is
                   indistinguishable from the distribution $Y$ using an
                   adversarial loss. Because this mapping is highly
                   under-constrained, we couple it with an inverse mapping $F:
                   Y \rightarrow X$ and introduce a cycle consistency loss to
                   push $F(G(X)) \approx X$ (and vice versa). Qualitative
                   results are presented on several tasks where paired training
                   data does not exist, including collection style transfer,
                   object transfiguration, season transfer, photo enhancement,
                   etc. Quantitative comparisons against several prior methods
                   demonstrate the superiority of our approach.",
  month         =  mar,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1703.10593"
}


@Comment [4] in Google Docs
@Comment [5] in Google Docs
@Comment [6] in Google Docs
@Comment [7] in Google Docs
@Comment [8] in Google Docs
@Comment [9] in Google Docs
@Comment [10] in Google Docs
@Comment [11] in Google Docs